{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing dataset\n",
    "data_path = r'D:\\Downloads\\cats-faces-64x64-for-generative-models\\cats'\n",
    "\n",
    "# Dimensions of the images inside the dataset\n",
    "img_dimensions = (64,64,3)\n",
    "\n",
    "# Folder where you want to save to model as well as generated samples\n",
    "model_path = r\"C:\\Users\\Vee\\Desktop\\python\\GAN\\pca_new\"\n",
    "\n",
    "# How many epochs between saving your model\n",
    "interval = 5\n",
    "\n",
    "# How many epochs to run the model\n",
    "epoch = 500\n",
    "\n",
    "# How many images to train at one time. If batch size is less than 8, alter the save_img function to plot less images\n",
    "# Ideally this number would be a factor of the size of your dataset\n",
    "batch = 181\n",
    "\n",
    "# How many convolutional filters for each convolutional layer of the generator and the discrminator\n",
    "conv_filters = 64\n",
    "\n",
    "# Size of kernel used in the convolutional layers\n",
    "kernel = (5,5)\n",
    "\n",
    "# Boolean flag, set to True if the data has pngs to remove alpha layer from images\n",
    "png = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Deep Convolutional GAN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    \n",
    "    # Initialize parameters, generator, and discriminator models\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Set dimensions of the output image\n",
    "        self.img_rows = img_dimensions[0]\n",
    "        self.img_cols = img_dimensions[1]\n",
    "        self.channels = img_dimensions[2]\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        # Set dimensions of the input noise\n",
    "        self.latent_dim = 512\n",
    "        \n",
    "        # Chose optimizer for the models\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        generator = self.generator\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    # load data from specified file path\n",
    "    def load_data(self):\n",
    "    \n",
    "        # Initializing arrays for data and image file paths\n",
    "        data = []\n",
    "        paths = []\n",
    "        \n",
    "        # Get the file paths of all jpg files in this folder\n",
    "        for r, d, f in os.walk(data_path):\n",
    "            for file in f:\n",
    "                if '.jpg' in file:\n",
    "                    paths.append(os.path.join(r, file))\n",
    "\n",
    "        # For each file add it to the data array\n",
    "        for path in paths:\n",
    "            \n",
    "            img = Image.open(path)\n",
    "            img = np.array(img.resize((self.img_rows, self.img_cols)))\n",
    "            \n",
    "            # Remove alpha layer if imgaes are PNG\n",
    "            if(png):\n",
    "                img = img[...,:3]\n",
    "                \n",
    "            data.append(img)\n",
    "        \n",
    "        #Reshaping data to be two dimensional for Principal Component Analysis\n",
    "        img_vector = np.array(data).reshape(len(data), self.img_rows * self.img_cols * self.channels)/255\n",
    "        \n",
    "        #Keep the first 512 eigenvectors of the covariance matrix of the img_vector\n",
    "        pca = PCA(n_components=512).fit(img_vector)\n",
    "        pca_data = pca.transform(img_vector)\n",
    "        \n",
    "        # Return x_train reshaped to two dimensions and Y_train reshaped to 4 dimensions\n",
    "        x_train = pca_data.reshape(len(pca_data), 512)\n",
    "        y_train = np.array(data)\n",
    "        y_train = y_train.reshape(len(data), self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        # Shuffle indexes of data\n",
    "        X_shuffle, Y_shuffle = shuffle(x_train, y_train)\n",
    "        \n",
    "        return X_shuffle, Y_shuffle\n",
    "    \n",
    "    # Define Generator model. There are 5 convolutional filters, upsampling from (8x8x8) to (64x64x3)\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input Layer\n",
    "        model.add(Dense(8 * 8 * 8, input_dim=self.latent_dim))\n",
    "        model.add(Reshape((8, 8, 8)))\n",
    "        \n",
    "        # 1st Convolutional Layer\n",
    "        model.add(Conv2D(conv_filters, kernel_size=kernel, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # Upsample the data (8x8 to 16x16)\n",
    "        model.add(UpSampling2D())\n",
    "        \n",
    "        # 2nd Convolutional Layer\n",
    "        model.add(Conv2D(conv_filters, kernel_size=kernel, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # Upsample the data (16x16 to 32x32)\n",
    "        model.add(UpSampling2D())\n",
    "\n",
    "        # 3rd Convolutional Layer\n",
    "        model.add(Conv2D(conv_filters, kernel_size=kernel, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # Upsample the data (32x32 to 64x64)\n",
    "        model.add(UpSampling2D())\n",
    "\n",
    "        # 4th Convolutional Layer\n",
    "        model.add(Conv2D(conv_filters, kernel_size=kernel, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # 5th Convolutional Layer (Output Layer)\n",
    "        model.add(Conv2D(3, kernel_size=kernel, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    # Define Discriminator model. There are 5 convolutional filters, downsampling from (64x64x3) to (1) scalar prediction\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input Layer\n",
    "        model.add(Conv2D(conv_filters, kernel_size=kernel, input_shape=self.img_shape,activation = \"relu\", padding=\"same\"))\n",
    "        \n",
    "        # Downsample the data (64x64 to 32x32)\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # 1st Convolutional Layer\n",
    "        model.add(Conv2D(conv_filters, kernel_size=kernel, activation='relu', padding=\"same\"))\n",
    "        \n",
    "        # Downsample the data (32x32 to 16x16)\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # 2nd Convolutional Layer\n",
    "        model.add(Conv2D(conv_filters, kernel_size=kernel, activation='relu', padding=\"same\"))\n",
    "        \n",
    "        # Downsample the data (16x16 to 8x8)\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # 3rd Convolutional Layer\n",
    "        model.add(Conv2D(conv_filters, kernel_size=kernel, activation='relu', padding=\"same\"))\n",
    "        \n",
    "        # 4th Convolutional Layer\n",
    "        model.add(Conv2D(conv_filters, kernel_size=kernel, activation='relu', padding=\"same\"))\n",
    "        \n",
    "        # 5th Convolutional Layer\n",
    "        model.add(Conv2D(conv_filters, kernel_size=kernel, activation='relu', padding=\"same\"))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # Output Layer\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "    \n",
    "    # Train the Generative Adversarial Network\n",
    "    def train(self, epochs, batch_size, save_interval):\n",
    "\n",
    "        # Load the dataset\n",
    "        X_train, Y_train = self.load_data()\n",
    "        \n",
    "        # Normalizing data to be between 0 and 1\n",
    "        Y_train = Y_train/255\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        # Placeholder arrays for Loss function values\n",
    "        g_loss_epochs = np.zeros((epochs, 1))\n",
    "        d_loss_epochs = np.zeros((epochs, 1))\n",
    "        \n",
    "        # Training the GAN\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            \n",
    "            # Initialize indexes for training data\n",
    "            start = 0\n",
    "            end = start + batch_size\n",
    "            \n",
    "            # Array to sum up all loss function values\n",
    "            discriminator_loss_real = []\n",
    "            discriminator_loss_fake = []\n",
    "            generator_loss = []\n",
    "            \n",
    "            # Iterate through dataset training one batch at a time\n",
    "            for i in range(int(len(X_train)/batch_size)):\n",
    "                \n",
    "                # Get batch of images\n",
    "                imgs = Y_train[start:end]\n",
    "                noise = X_train[start:end]\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Make predictions on current batch using generator\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator (real classified as ones and generated as zero)\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator (wants discriminator to mistake images as real)\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "                \n",
    "                # Add loss for current batch to sum over entire epoch\n",
    "                discriminator_loss_real.append(d_loss[0])\n",
    "                discriminator_loss_fake.append(d_loss[1])\n",
    "                generator_loss.append(g_loss)\n",
    "                \n",
    "                # Increment image indexes\n",
    "                start = start + batch_size\n",
    "                end = end + batch_size\n",
    "             \n",
    "            \n",
    "            # Get average loss over the entire epoch\n",
    "            loss_data = [np.average(discriminator_loss_real),np.average(discriminator_loss_fake),np.average(generator_loss)]\n",
    "            \n",
    "            #save loss history\n",
    "            g_loss_epochs[epoch - 1] = loss_data[2]\n",
    "            \n",
    "            # Average loss of real data classification and fake data accuracy\n",
    "            d_loss_epochs[epoch - 1] = (loss_data[0] + (1 - loss_data[1])) / 2\n",
    "                \n",
    "            # Print average loss over current epoch\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, loss_data[0], loss_data[1]*100, loss_data[2]))\n",
    "\n",
    "            # If epoch is at intervale, save model and generate image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                \n",
    "                # Select 8 random indexes\n",
    "                idx = np.random.randint(0, X_train.shape[0], 8)\n",
    "                # Get batch of generated images and training images\n",
    "                x_points = X_train[idx]\n",
    "                y_points = Y_train[idx]\n",
    "                \n",
    "                # Plot the predictions next to the training imgaes\n",
    "                self.save_imgs(epoch, self.generator.predict(x_points), y_points)\n",
    "                \n",
    "        return g_loss_epochs, d_loss_epochs\n",
    "    \n",
    "    # Save the model and generate prediction samples for a given epoch\n",
    "    def save_imgs(self, epoch, gen_imgs, y_points):\n",
    "        \n",
    "        # Define number of columns and rows\n",
    "        r, c = 4, 4\n",
    "        \n",
    "        # Placeholder array for MatPlotLib Figure Subplots\n",
    "        subplots = []\n",
    "        \n",
    "        # Unnormalize data to be between 0 and 255 for RGB image\n",
    "        gen_imgs = np.array(gen_imgs) * 255\n",
    "        gen_imgs = gen_imgs.astype(int)\n",
    "        y_points = np.array(y_points) * 255\n",
    "        y_points = y_points.astype(int)\n",
    "        \n",
    "        # Create figure with title\n",
    "        fig = plt.figure(figsize= (40, 40))\n",
    "        fig.suptitle(\"Epoch: \" + str(epoch), fontsize=65)\n",
    "        \n",
    "        # Initialize counters needed to track indexes across multiple arrays\n",
    "        img_count = 0;\n",
    "        index_count = 0;\n",
    "        y_count = 0;\n",
    "        \n",
    "        # Loop through columns and rows of the figure\n",
    "        for i in range(1, c+1):\n",
    "            for j in range(1, r+1):\n",
    "                # If row is even, plot the predictions\n",
    "                if(j % 2 == 0):\n",
    "                    img = gen_imgs[index_count]\n",
    "                    index_count = index_count + 1\n",
    "                # If row is odd, plot the training image\n",
    "                else:\n",
    "                    img = y_points[y_count]\n",
    "                    y_count = y_count + 1\n",
    "                # Add image to figure, add subplot to array\n",
    "                subplots.append(fig.add_subplot(r, c, img_count + 1))\n",
    "                plt.imshow(img)\n",
    "                img_count = img_count + 1\n",
    "        \n",
    "        # Add title to columns of figure\n",
    "        subplots[0].set_title(\"Training\", fontsize=45)\n",
    "        subplots[1].set_title(\"Predicted\", fontsize=45)\n",
    "        subplots[2].set_title(\"Training\", fontsize=45)\n",
    "        subplots[3].set_title(\"Predicted\", fontsize=45)\n",
    "                \n",
    "        # Save figure to .png image in specified folder\n",
    "        fig.savefig(model_path + \"\\\\epoch_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "        \n",
    "        # save model to .h5 file in specified folder\n",
    "        self.generator.save(model_path + \"\\\\generator\" + str(epoch) + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Generator and Discriminator Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          102464    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          102464    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 64)          102464    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 521,281\n",
      "Trainable params: 521,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 64)          12864     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 64, 64, 3)         4803      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 587,715\n",
      "Trainable params: 587,715\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.697253, acc.: 52.06%] [G loss: 0.703769]\n",
      "2 [D loss: 0.694933, acc.: 53.72%] [G loss: 0.705698]\n",
      "3 [D loss: 0.692058, acc.: 53.39%] [G loss: 0.708609]\n",
      "4 [D loss: 0.691899, acc.: 54.56%] [G loss: 0.706116]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [D loss: 0.690893, acc.: 55.82%] [G loss: 0.712449]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "g_loss, d_loss = dcgan.train(epochs=epoch, batch_size=batch, save_interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_loss)\n",
    "plt.plot(d_loss)\n",
    "plt.title('GAN Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Generator', 'Discriminator'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
